{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9379034-43ae-4c6c-8879-631bd55888d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b51412f-a4e1-4f63-aa86-819ac3338a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37354395-5fa7-4a1b-add7-3c846a58341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define utility functions\n",
    "\n",
    "def apply_field(img, field):\n",
    "    \"\"\"Applies field to a fixed image to estimate a target image.\"\"\"\n",
    "    \n",
    "    # create identity coordinate grid\n",
    "    grid = torch.stack(torch.meshgrid(torch.arange(28), torch.arange(28)), dim=-1).float()\n",
    "    grid = grid.unsqueeze(0)  # Add batch dimension\n",
    "    grid = grid.repeat(img.shape[0], 1, 1, 1)\n",
    "    grid = (grid / 28) * 2 - 1\n",
    "    \n",
    "    # add field to grid\n",
    "    field = field.permute(0,2,3,1)\n",
    "    coords_adj = grid + field\n",
    "    coords_adj = coords_adj.float().permute(0,2,1,3)\n",
    "\n",
    "    # adjust image\n",
    "    img_adj = F.grid_sample(img, coords_adj, mode=\"bilinear\", align_corners=True)\n",
    "    \n",
    "    return(img_adj)\n",
    "\n",
    "def loss(img1, img2, field, lmbda):\n",
    "    \"\"\"Calculates loss associated with image reconstruction and associated field.\"\"\"\n",
    "    \n",
    "    # approximate field gradient\n",
    "    diff_x = torch.diff(field[:,:,:,0], axis=1)\n",
    "    diff_y = torch.diff(field[:,:,:,1], axis=2)\n",
    "    diff_x = F.pad(diff_x, (0, 0, 1, 0), mode='constant')\n",
    "    diff_y = F.pad(diff_y, (1, 0, 0, 0), mode='constant')\n",
    "\n",
    "    # calculate loss\n",
    "    loss_sim = torch.sum((img1 - img2)**2)\n",
    "    loss_smooth = torch.sum(diff_x**2 + diff_y**2)\n",
    "    loss_total = loss_sim + lmbda * loss_smooth\n",
    "    return loss_total\n",
    "\n",
    "def show_images(img, img_adj, img_goal):  \n",
    "    \"\"\"Utility for displaying fixed, estimated, and target images.\"\"\"\n",
    "    # convert pytorch to numpy\n",
    "    img = img.detach().numpy()\n",
    "    img_adj = img_adj.detach().numpy()\n",
    "    img_goal = img_goal.detach().numpy()    \n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title('Original Image')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(img_adj)\n",
    "    plt.title('Estimated Image')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(img_goal)\n",
    "    plt.title('Goal Image')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75a8bd88-2903-4a9b-be1b-172afb413e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "\n",
    "class DeformatioNet(nn.Module):\n",
    "    def __init__(self, n_1, n_2):\n",
    "        super(DeformatioNet, self).__init__()\n",
    "        \n",
    "        self.Encoder = nn.Sequential(\n",
    "                nn.Conv2d(2, n_1, 3, stride=1, padding=2),\n",
    "                nn.BatchNorm2d(n_1),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(n_1, n_2, 3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(n_2),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(n_2, n_2, 3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(n_2),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "            )\n",
    "\n",
    "        self.Decoder = nn.Sequential(\n",
    "                nn.ConvTranspose2d(n_2, n_2, 3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(n_2),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.ConvTranspose2d(n_2, n_1, 3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(n_1),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Upsample(scale_factor=2, mode=\"bicubic\", align_corners=True),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.ConvTranspose2d(n_1, 2, 3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(2),\n",
    "                nn.Upsample(scale_factor=2, mode=\"bicubic\", align_corners=True),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # stack images\n",
    "        z = torch.cat((x,y),\n",
    "                      dim=1)\n",
    "        \n",
    "        # encode images into latent space\n",
    "        enc = self.Encoder(z)\n",
    "        \n",
    "        # decode latent space into deformation field\n",
    "        field = self.Decoder(enc)\n",
    "\n",
    "        # (will use to adjust image in post)\n",
    "        return(field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f847a-885e-4f6f-9915-3627b0b3e729",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1bba5a8-bc6a-4c77-9ab9-e429ee6bac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "epochs = 40\n",
    "batch_size = 179\n",
    "val_iter = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4dd6e4c3-1d17-4934-ab0d-05833616a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "# load training data\n",
    "train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train.data = train.data[train.targets == 7]\n",
    "train.targets = train.targets[train.targets == 7]\n",
    "\n",
    "# split into training/validation sets\n",
    "train, val = random_split(train, [int(0.8 * len(train)), len(train) - int(0.8 * len(train))])\n",
    "\n",
    "# load test data\n",
    "test = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test.data = test.data[test.targets == 7]\n",
    "test.targets = test.targets[test.targets == 7]\n",
    "test_loader = DataLoader(test, batch_size=1028, shuffle=True)\n",
    "test_loader2 = DataLoader(test, batch_size=1028, shuffle=True)\n",
    "\n",
    "# load more test data\n",
    "test4 = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test4.data = test4.data[test4.targets == 4]\n",
    "test4.targets = test4.targets[test4.targets == 4]\n",
    "test4_loader = DataLoader(test4, batch_size=982, shuffle=True)\n",
    "test4_loader2 = DataLoader(test4, batch_size=982, shuffle=True)\n",
    "\n",
    "test6 = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test6.data = test6.data[test6.targets == 6]\n",
    "test6.targets = test6.targets[test6.targets == 6]\n",
    "test6_loader = DataLoader(test6, batch_size=982, shuffle=True)\n",
    "test6_loader2 = DataLoader(test6, batch_size=982, shuffle=True)\n",
    "\n",
    "test1 = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test1.data = test1.data[test1.targets == 1]\n",
    "test1.targets = test1.targets[test1.targets == 1]\n",
    "test1_loader = DataLoader(test1, batch_size=982, shuffle=True)\n",
    "test1_loader2 = DataLoader(test1, batch_size=982, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2951bb0f-b0a4-40a1-a69d-98cf62ab58a9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "def train_model(model, losses,\n",
    "                train, val,\n",
    "                val_iter, \n",
    "                lmbda, lmbda2, log_lr):\n",
    "    \"\"\"Trains model for a single epoch.\"\"\"\n",
    "    \n",
    "    base_batch = len(losses)\n",
    "    \n",
    "    # set up data loaders\n",
    "    torch.manual_seed(random.randint(0, 100000))\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    torch.manual_seed(random.randint(0, 100000))\n",
    "    train_loader2 = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    torch.manual_seed(random.randint(0, 100000))\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "    torch.manual_seed(random.randint(0, 100000))\n",
    "    val_loader2 = DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # set optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=10**log_lr, weight_decay=lmbda2)\n",
    "    \n",
    "    # run training loop\n",
    "    \n",
    "    for batch_id, (img, _) in enumerate(train_loader):\n",
    "        print(f\"Batch {batch_id+1} of {len(train_loader)}\", end=\"\\r\")\n",
    "        _, (img_goal, _) = next(enumerate(train_loader2))\n",
    "        \n",
    "        # training step\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        field = model(img, img_goal)\n",
    "        \n",
    "        img_adj = apply_field(img, field)\n",
    "        \n",
    "        loss_train = loss(img_adj, img_goal, field, lmbda)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # validation step\n",
    "        loss_val = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(val_iter):\n",
    "                _, (img_val, _) = next(enumerate(val_loader))\n",
    "                _, (img_val_goal, _) = next(enumerate(val_loader2))\n",
    "                \n",
    "                field_val = model(img_val, img_val_goal)\n",
    "                img_val_adj = apply_field(img_val, field_val)\n",
    "                loss_val += loss(img_val_adj, img_val_goal, field_val, lmbda)\n",
    "            loss_val = loss_val/val_iter\n",
    "\n",
    "        losses.append([base_batch + batch_id, \n",
    "                       torch.log(loss_train).item(), torch.log(loss_val).item()])    \n",
    "        \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a8b08-fe57-416e-b808-9d7bf4838172",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "\n",
    "lmbdas = [-1, -2, -3]\n",
    "lmbdas2 = [-1]\n",
    "log_lrs = [-1, -2, -3]\n",
    "n_1s = [256]\n",
    "n_2s = [256]\n",
    "\n",
    "i = 0\n",
    "for n_2 in n_2s:\n",
    "    for n_1 in n_1s:\n",
    "        for log_lr in log_lrs:\n",
    "            for lmbda in lmbdas:\n",
    "                for lmbda2 in lmbdas2:\n",
    "                    i += 1\n",
    "\n",
    "                    # load hyperparameters\n",
    "                    # skip if already tested\n",
    "                    if os.path.exists(\"hyperparameters.pth\"):\n",
    "                        params_load = torch.load(\"hyperparameters.pth\")\n",
    "                        losses_load = torch.load(\"losses.pth\")\n",
    "                    else:\n",
    "                        params_load = torch.empty(0,5)\n",
    "                        losses_load = torch.empty(28*epochs,3,0)\n",
    "\n",
    "                    row_match = (params_load[:, 0] == lmbda) & (params_load[:, 1] == lmbda2) & (params_load[:, 2] == log_lr) & (params_load[:, 3] == n_1) & (params_load[:, 4] == n_2)\n",
    "                    if torch.any(row_match):\n",
    "                        continue\n",
    "\n",
    "                    # run training loop\n",
    "                    model = DeformatioNet(n_1, n_2)\n",
    "\n",
    "                    print(f\"Combination {i} of {len(lmbdas)*len(lmbdas2)*len(log_lrs)*len(n_1s)*len(n_2s)}\")\n",
    "                    print(f\"Current parameters: lmbda={lmbda}; lmbda2={lmbda2}, log_lr={log_lr}; n_1={n_1}; n_2={n_2}\")\n",
    "\n",
    "                    losses = []\n",
    "                    for epoch in range(epochs):\n",
    "                        print(f\"Epoch {epoch+1} of {epochs}     \")\n",
    "                        model, losses = train_model(model, losses,\n",
    "                                                    train, val,\n",
    "                                                    val_iter, \n",
    "                                                    10**lmbda, 10**lmbda2, log_lr)\n",
    "\n",
    "\n",
    "                    # save tested hyperparameters\n",
    "                    losses_save = torch.Tensor(losses).unsqueeze(2)\n",
    "                    params_save = torch.Tensor([lmbda, lmbda2, log_lr, n_1, n_2]).unsqueeze(0)\n",
    "\n",
    "                    losses_out = torch.cat((losses_load, losses_save), dim=2)\n",
    "                    params_out = torch.cat((params_load, params_save), dim=0)\n",
    "\n",
    "                    torch.save(losses_out, \"losses.pth\")\n",
    "                    torch.save(params_out, \"hyperparameters.pth\")\n",
    "                    print(\"Test saved!                    \\n\")\n",
    "\n",
    "# plot loss\n",
    "losses_out = losses_save\n",
    "plt.plot(losses_out[:,0], losses_out[:,1], label='Training')\n",
    "plt.plot(losses_out[:,0], losses_out[:,2], label='Validation')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log-loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673fb2a2-3dbd-42fe-a6f5-16eaf20cf319",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in best hyperparameters\n",
    "\n",
    "params = torch.load(\"hyperparameters.pth\")\n",
    "losses = torch.load(\"losses.pth\")\n",
    "\n",
    "mins, _ = torch.min(losses[:, 2, :], dim=0)\n",
    "min_all, ind = torch.min(mins, dim=0)\n",
    "\n",
    "lmbda_best, lmbda2_best, log_lr_best, n_1_best, n_2_best = params[ind,:]\n",
    "log_lr_best = int(log_lr_best)\n",
    "n_1_best = int(n_1_best)\n",
    "n_2_best = int(n_2_best)\n",
    "\n",
    "print(f\"Best model: lmbda={lmbda_best}, lmbda2={lmbda2_best}, log_lr={log_lr_best}, n_1={n_1_best}, n_2={n_2_best}\")\n",
    "print(f\"Combination {ind} of {len(params)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c97ca-dcfb-4c8d-adc4-98dd6ec10009",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train tuned model\n",
    "\n",
    "## bested model parameters\n",
    "#lmbda_best = -3\n",
    "#lmbda2_best = -1\n",
    "#n_1_best = 256\n",
    "#n_2_best = 256\n",
    "#log_lr_best = -1\n",
    "#epochs = 20\n",
    "\n",
    "model = DeformatioNet(n_1_best, n_2_best)\n",
    "test_epochs = 40\n",
    "\n",
    "best_path = \"best_state256.pth\"\n",
    "if os.path.exists(best_path):\n",
    "    model.load_state_dict(torch.load(best_path))\n",
    "    losses = torch.load(\"best_losses256.pth\")\n",
    "    min_epoch = ((losses[-1][0]+1)/28).int()\n",
    "    losses = losses.tolist()\n",
    "else:\n",
    "    min_epoch = 0\n",
    "    losses = []\n",
    "\n",
    "for epoch in range(min_epoch, test_epochs):\n",
    "    print(f\"Epoch {epoch+1} of {test_epochs}     \")\n",
    "    model, losses = train_model(model, losses,\n",
    "                                train, val,\n",
    "                                val_iter, \n",
    "                                10**lmbda_best, 10**lmbda2_best, log_lr_best)\n",
    "    torch.save(model.state_dict(), \"best_state256.pth\")\n",
    "    torch.save(torch.Tensor(losses), \"best_losses256.pth\")\n",
    "    \n",
    "    # early stopping\n",
    "    mean_val_loss = torch.mean(torch.Tensor(losses)[-27:-1, -1])\n",
    "    mean_val_loss_prev = torch.mean(torch.Tensor(losses)[-55:-28, -1])\n",
    "    if (epoch > 1) & (mean_val_loss > mean_val_loss_prev):\n",
    "        break\n",
    "    \n",
    "torch.save(model.state_dict(), \"best_state256.pth\")\n",
    "torch.save(torch.Tensor(losses), \"best_losses256.pth\")\n",
    "\n",
    "losses_out = torch.Tensor(losses)\n",
    "plt.plot(losses_out[:,0], losses_out[:,1], label='Training')\n",
    "plt.plot(losses_out[:,0], losses_out[:,2], label='Validation')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log-loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea900096-3996-4945-b775-f834987f506a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b631cf-b4d1-461b-a2d9-caa52400bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "train_loader2 = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# set up demo images\n",
    "img, _ = next(iter(test_loader))\n",
    "img_goal, _ = next(iter(test_loader2))\n",
    "\n",
    "img_train, _ = next(iter(train_loader))\n",
    "img_goal_train, _ = next(iter(train_loader2))\n",
    "\n",
    "# generate deformation field\n",
    "field = model(img, img_goal)\n",
    "field_train = model(img_train, img_goal_train)\n",
    "\n",
    "img_adj = apply_field(img, field)\n",
    "img_adj_train = apply_field(img_train, field_train)\n",
    "\n",
    "\n",
    "# display original/transformed images\n",
    "# test dataset\n",
    "field_out = field[0,:,:,:]\n",
    "field_out = field_out.detach().numpy()\n",
    "\n",
    "x, y = torch.meshgrid(torch.arange(28), torch.arange(28))\n",
    "plt.quiver(x, y, field_out[0], field_out[1], scale=30)\n",
    "plt.show()\n",
    "\n",
    "show_images(img[0, 0, :, :], img_adj[0, 0, :, :], img_goal[0, 0, :, :])\n",
    "\n",
    "\n",
    "# training dataset\n",
    "field_out_train = field_train[0,:,:,:]\n",
    "field_out_train = field_out_train.detach().numpy()\n",
    "\n",
    "plt.quiver(x, y, field_out_train[0], field_out_train[1], scale=30)\n",
    "plt.show()\n",
    "\n",
    "show_images(img_train[0, 0, :, :], img_adj_train[0, 0, :, :], img_goal_train[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60cb3c4-d588-447b-b562-319267652435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalization\n",
    "img4, _ = next(iter(test4_loader))\n",
    "img4_goal, _ = next(iter(test4_loader2))\n",
    "\n",
    "# generate deformation field\n",
    "field4 = model(img4, img4_goal)\n",
    "\n",
    "img4_adj = apply_field(img4, field4)\n",
    "\n",
    "# display original/transformed images\n",
    "\n",
    "# test dataset\n",
    "field_out = field4[0,:,:,:]\n",
    "field_out = field_out.detach().numpy()\n",
    "\n",
    "x, y = torch.meshgrid(torch.arange(28), torch.arange(28))\n",
    "plt.quiver(x, y, field_out[0], field_out[1], scale=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65aa0a3-7188-4e28-a198-e366220e1610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalization\n",
    "img6, _ = next(iter(test6_loader))\n",
    "img6_goal, _ = next(iter(test6_loader2))\n",
    "\n",
    "# generate deformation field\n",
    "field6 = model(img6, img6_goal)\n",
    "\n",
    "img6_adj = apply_field(img6, field6)\n",
    "\n",
    "# display original/transformed images\n",
    "\n",
    "# test dataset\n",
    "field_out = field6[0,:,:,:]\n",
    "field_out = field_out.detach().numpy()\n",
    "\n",
    "x, y = torch.meshgrid(torch.arange(28), torch.arange(28))\n",
    "plt.quiver(x, y, field_out[0], field_out[1], scale=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f940ce-f829-4fc2-8b4a-ca60406ce287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalization\n",
    "img1, _ = next(iter(test1_loader))\n",
    "img1_goal, _ = next(iter(test1_loader2))\n",
    "\n",
    "# generate deformation field\n",
    "field1 = model(img1, img_goal)\n",
    "\n",
    "img1_adj = apply_field(img1, field1)\n",
    "\n",
    "# display original/transformed images\n",
    "\n",
    "# test dataset\n",
    "field_out = field1[0,:,:,:]\n",
    "field_out = field_out.detach().numpy()\n",
    "\n",
    "x, y = torch.meshgrid(torch.arange(28), torch.arange(28))\n",
    "plt.quiver(x, y, field_out[0], field_out[1], scale=30)\n",
    "plt.show()\n",
    "\n",
    "show_images(img1[0, 0, :, :], img1_adj[0, 0, :, :], img1_goal[0, 0, :, :])\n",
    "show_images(img4[0, 0, :, :], img4_adj[0, 0, :, :], img4_goal[0, 0, :, :])\n",
    "show_images(img6[0, 0, :, :], img6_adj[0, 0, :, :], img6_goal[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e436e0-25a5-4040-9ef5-faa6e716030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test error\n",
    "\n",
    "# mean loss: 7s\n",
    "_, (img, _) = next(enumerate(test_loader))\n",
    "_, (img_goal, _) = next(enumerate(test_loader2))\n",
    "field = model(img, img_goal)\n",
    "img_adj = apply_field(img, field)\n",
    "loss7 = loss(img_adj, img_goal, field, lmbda)/len(test)\n",
    "\n",
    "# mean loss: 4s\n",
    "_, (img, _) = next(enumerate(test4_loader))\n",
    "_, (img_goal, _) = next(enumerate(test4_loader2))\n",
    "field = model(img, img_goal)\n",
    "img_adj = apply_field(img, field)\n",
    "loss4 = loss(img_adj, img_goal, field, lmbda)/len(test4)\n",
    "\n",
    "# mean loss: 6s\n",
    "_, (img, _) = next(enumerate(test6_loader))\n",
    "_, (img_goal, _) = next(enumerate(test6_loader2))\n",
    "field = model(img, img_goal)\n",
    "img_adj = apply_field(img, field)\n",
    "loss6 = loss(img_adj, img_goal, field, lmbda)/len(test6)\n",
    "\n",
    "# mean loss: 4s\n",
    "_, (img, _) = next(enumerate(test1_loader))\n",
    "_, (img_goal, _) = next(enumerate(test1_loader2))\n",
    "field = model(img, img_goal)\n",
    "img_adj = apply_field(img, field)\n",
    "loss1 = loss(img_adj, img_goal, field, lmbda)/len(test1)\n",
    "\n",
    "bars = plt.bar([0,1,2,3], [loss7.item(), loss1.item(), loss4.item(), loss6.item()])\n",
    "bars[0].set_color('orange')\n",
    "plt.ylabel('Mean Loss')\n",
    "plt.xlabel('Number')\n",
    "plt.xticks([0, 1, 2, 3], ['7', '1', '4', '6'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
