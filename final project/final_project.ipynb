{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9379034-43ae-4c6c-8879-631bd55888d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b51412f-a4e1-4f63-aa86-819ac3338a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "47090d95-1038-4869-ae79-a587e8ad9a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define utility functions\n",
    "\n",
    "def apply_field(img, field):\n",
    "    \"\"\"Applies specified deformation to an image.\"\"\"\n",
    "    img = img.numpy()\n",
    "    print(img.shape)\n",
    "    \n",
    "    # set up adjusted coordinates\n",
    "    _, _, h, w = img.shape\n",
    "    x, y = np.meshgrid(np.arange(w), np.arange(h))\n",
    "    coords = np.stack((x, y), axis=-1)\n",
    "    coords_adj = coords + field\n",
    "\n",
    "    # remap pixels\n",
    "    img_adj = cv2.remap(img, coords_adj[:,:,0].astype(np.float32),\n",
    "                        coords_adj[:,:,1].astype(np.float32), \n",
    "                        interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    img_adj = torch.Tensor(img_adj)\n",
    "    return(img_adj)\n",
    "\n",
    "def loss(img1, img2, field, lmbda):\n",
    "    \"\"\"Calculates loss associated with image reconstruction and associated field.\"\"\"\n",
    "    \n",
    "    # approximate field gradient\n",
    "    diff_x = torch.diff(field[:,:,0], axis=0)\n",
    "    diff_y = torch.diff(field[:,:,1], axis=1)\n",
    "    diff_x = F.pad(diff_x, (0, 0, 1, 0), mode='constant')\n",
    "    diff_y = F.pad(diff_y, (1, 0, 0, 0), mode='constant')\n",
    "\n",
    "    # calculate loss\n",
    "    loss_sim = torch.sum((img1 - img2)**2)\n",
    "    loss_smooth = torch.sum(diff_x**2 + diff_y**2)\n",
    "    loss_total = loss_sim + lmbda * loss_smooth\n",
    "    return(loss_total.item())\n",
    "\n",
    "def show_images(img, img_adj, img_goal):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title('Original Image')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(img_adj)\n",
    "    plt.title('Estimated Image')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(img_goal)\n",
    "    plt.title('Goal Image')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "022ac27f-7622-4ced-a53b-1b341079f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "\n",
    "class DeformatioNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeformatioNet, self).__init__()\n",
    "        \n",
    "        self.Encoder = nn.Sequential(\n",
    "                nn.Conv2d(2, 16, 2, stride=2),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "            )\n",
    "\n",
    "        self.Decoder = nn.Sequential(\n",
    "                nn.ConvTranspose2d(16, 2, 2, stride=2),\n",
    "                nn.BatchNorm2d(2),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # stack images\n",
    "        z = torch.cat((x,y),\n",
    "                      dim=1)\n",
    "        \n",
    "        # encode images into latent space\n",
    "        enc = self.Encoder(z)\n",
    "        \n",
    "        # decode latent space into deformation field\n",
    "        field = self.Decoder(enc)*28 # multiply by 28 to fit image dimensions\n",
    "\n",
    "        # (will use to adjust image in post)\n",
    "        return(field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f847a-885e-4f6f-9915-3627b0b3e729",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "02e45fef-f2fd-4c56-85e3-49ac0f16db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "field = model(img, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1bba5a8-bc6a-4c77-9ab9-e429ee6bac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "batch_size = 1\n",
    "lmbda = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6a80afb-88bd-4577-b64d-920b993ed336",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "# load training data\n",
    "train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train.data = train.data[train.targets == 7]\n",
    "train.targets = train.targets[train.targets == 7]\n",
    "\n",
    "# split into training/validation sets\n",
    "train, val = random_split(train, [int(0.8 * len(train)), len(train) - int(0.8 * len(train))])\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "train_loader2 = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "val_loader2 = DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# load test data\n",
    "test = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test.data = test.data[test.targets == 7]\n",
    "test.targets = test.targets[test.targets == 7]\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "test_loader2 = DataLoader(test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3a1425dc-e653-4309-80d9-2772e59928db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "torch.Size([1, 2, 28, 28])\n",
      "(1, 1, 28, 28)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Concatenation operation is not implemented for NumPy arrays, use np.concatenate() instead. Please do not rely on this error; it may not be given on all Python implementations.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [82]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m DeformatioNet()\n\u001b[0;32m---> 30\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [82]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, train_loader2, lmbda, log_lr)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(field\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 21\u001b[0m img_adj \u001b[38;5;241m=\u001b[39m \u001b[43mapply_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m loss(img_adj, img_goal, field, lmbda)\n\u001b[1;32m     24\u001b[0m loss_train\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Input \u001b[0;32mIn [80]\u001b[0m, in \u001b[0;36mapply_field\u001b[0;34m(img, field)\u001b[0m\n\u001b[1;32m     10\u001b[0m x, y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(np\u001b[38;5;241m.\u001b[39marange(w), np\u001b[38;5;241m.\u001b[39marange(h))\n\u001b[1;32m     11\u001b[0m coords \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack((x, y), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m coords_adj \u001b[38;5;241m=\u001b[39m \u001b[43mcoords\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfield\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# remap pixels\u001b[39;00m\n\u001b[1;32m     15\u001b[0m img_adj \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mremap(img, coords_adj[:,:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m     16\u001b[0m                     coords_adj[:,:,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32), \n\u001b[1;32m     17\u001b[0m                     interpolation\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_LINEAR)\n",
      "\u001b[0;31mTypeError\u001b[0m: Concatenation operation is not implemented for NumPy arrays, use np.concatenate() instead. Please do not rely on this error; it may not be given on all Python implementations."
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "def train_model(model, train_loader, train_loader2, lmbda, log_lr):\n",
    "    \n",
    "    # set optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=10**log_lr, weight_decay=0)\n",
    "    \n",
    "    # run training loop\n",
    "    for batch_id, (img, _) in enumerate(train_loader):\n",
    "        print(f\"Batch {batch_id+1} of {len(train_loader)}\", end=\"\\r\")\n",
    "        \n",
    "        _, (img_goal, _) = next(enumerate(train_loader2))\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        field = model(img, img_goal)\n",
    "        print(img.shape)\n",
    "        print(field.shape)\n",
    "        \n",
    "        img_adj = apply_field(img, field)\n",
    "        \n",
    "        loss_train = loss(img_adj, img_goal, field, lmbda)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = DeformatioNet()\n",
    "model = train_model(model, train_loader, train_loader2, 0.01, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439dba85-23fd-49b1-9cbb-c611ec8fd60e",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b50b893-6448-4228-b19c-7b24fd78c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random demo field\n",
    "field = np.random.uniform(-1, 1, (28, 28, 2))\n",
    "\n",
    "# set up demo images\n",
    "img, _ = next(iter(train_loader))\n",
    "img = img.squeeze()\n",
    "\n",
    "img_goal, _ = next(iter(train_loader2))\n",
    "img_goal = img_goal.squeeze()\n",
    "\n",
    "# apply deformation field\n",
    "field = model(img, img_goal).squeeze(0).detach().numpy()\n",
    "field = np.transpose(field, (1,2,0))\n",
    "img_adj = apply_field(img, field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a964095d-d69e-4ea4-84e9-b07544212e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display original/transformed images\n",
    "show_images(img, img_adj, img_goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d3f3b7b-56cc-4aec-8a2e-45363424725d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.15184783935547"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = torch.Tensor(img)\n",
    "img2 = torch.Tensor(img_adj)\n",
    "field2 = torch.Tensor(field)\n",
    "\n",
    "loss(img1, img2, field2, lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66535487-16d0-47bf-92a1-53f6b6ab1f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2456fd0d-9d28-4fb3-aca7-7279ac9f250d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
