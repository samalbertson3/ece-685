{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "131ae3ab-1d96-4dc7-a565-6fbd1a28be07",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "79a1beaf-580d-4c00-8676-9abb8e7915c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# Generate images with condition labels\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch import nn\n",
    "\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import time as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "6fe00a1d-502f-48e0-be2b-4fd7b1d10ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 1\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "56e9cf29-eb27-4562-8b4c-d4fbfded6bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Put your code here\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "id": "c58dab0b-2b27-4982-8b2e-47f55064031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratioNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(GeneratioNet, self).__init__()\n",
    "        \n",
    "        self.Decoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 256, 2, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 1, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward function should return batch of images.\"\"\"\n",
    "        \n",
    "        x = x.view(batch_size, 1, 10, 10)\n",
    "        x = self.Decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "id": "db5e06e5-1822-408d-8382-26c7221a9a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscrimiNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DiscrimiNet, self).__init__()\n",
    "\n",
    "        self.Encoder = nn.Sequential(\n",
    "                nn.Conv2d(1, 64, 3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Dropout(0.5),\n",
    "            \n",
    "                nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Dropout(0.5),\n",
    "            \n",
    "                nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "            \n",
    "                nn.Conv2d(256, 512, 3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "            )\n",
    "        \n",
    "        self.Regression = nn.Linear(512*4*4, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward function should return the logits.\"\"\"\n",
    "        \n",
    "        x = self.Encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.Regression(x)\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "id": "77d4f020-86cb-4c9d-b321-6cc3ea537380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGANet(object):\n",
    "    \n",
    "    def __init__(self, epochs, batch_size):\n",
    "        \n",
    "        ##### ---- YOUR CODE HERE ---- #####\n",
    "        self.G = GeneratioNet()\n",
    "        self.D = DiscrimiNet()\n",
    "        self.loss = nn.BCELoss()\n",
    "        ##### ----                ---- #####\n",
    "\n",
    "        self.d_optimizer = torch.optim.Adam(self.D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        self.g_optimizer = torch.optim.Adam(self.G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.number_of_images = 10\n",
    "        \n",
    "    def train(self, train_loader, disc_loss=None, genr_loss=None):\n",
    "        \n",
    "        if disc_loss is None:\n",
    "            disc_loss = []\n",
    "        if genr_loss is None:\n",
    "            genr_loss = []\n",
    "        \n",
    "        generator_iter = 0\n",
    "        \n",
    "        for epoch in trange(self.epochs):\n",
    "\n",
    "            for i, (images, _) in enumerate(train_loader):\n",
    "                print(f\"Epoch {epoch+1}: Batch {i+1} of {len(train_loader)}\")\n",
    "                \n",
    "                # Step 1: Train discriminator\n",
    "                z = torch.randn((self.batch_size, 100, 1, 1))\n",
    "                \n",
    "                real_labels = torch.ones(self.batch_size)\n",
    "                fake_labels = torch.zeros(self.batch_size)\n",
    "\n",
    "                images, z = images.to(device), z.to(device)\n",
    "                real_labels, fake_labels = real_labels.to(device), fake_labels.to(device)\n",
    "\n",
    "                # Compute the BCE Loss using real images\n",
    "                real_logits = self.D(images)\n",
    "                real_logits = torch.squeeze(real_logits)\n",
    "                d_loss_real = self.loss(real_logits, real_labels)\n",
    "\n",
    "                # Compute the BCE Loss using fake images\n",
    "                print(\"Generating images...\")\n",
    "                fake_images = self.G(z)\n",
    "                print(\"Discriminating images...\")\n",
    "                fake_logits = self.D(fake_images)\n",
    "                fake_logits = torch.squeeze(fake_logits)\n",
    "                d_loss_fake = self.loss(fake_logits, fake_labels)\n",
    "\n",
    "                # Optimize discriminator\n",
    "                d_loss = d_loss_real + d_loss_fake\n",
    "                self.D.zero_grad()\n",
    "                d_loss.backward()\n",
    "                self.d_optimizer.step()\n",
    "\n",
    "                # Step 2: Train Generator\n",
    "                z = torch.randn(self.batch_size, 100, 1, 1).to(device)\n",
    "                \n",
    "                fake_images = self.G(z)\n",
    "                fake_logits = self.D(fake_images)\n",
    "                fake_logits = torch.squeeze(fake_logits)\n",
    "                g_loss = self.loss(fake_logits, real_labels)\n",
    "\n",
    "                self.D.zero_grad()\n",
    "                self.G.zero_grad()\n",
    "                g_loss.backward()\n",
    "                self.g_optimizer.step()\n",
    "                generator_iter += 1\n",
    "\n",
    "                disc_loss.append(d_loss.item())\n",
    "                genr_loss.append(g_loss.item())\n",
    "\n",
    "        return disc_loss, genr_loss\n",
    "\n",
    "    def generate_img(self, z, number_of_images):\n",
    "        samples = self.G(z).data.cpu().numpy()[:number_of_images]\n",
    "        generated_images = []\n",
    "        for sample in samples:\n",
    "            generated_images.append(sample.reshape(28, 28))\n",
    "        return generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "id": "31b7c06f-e480-425a-8818-72cac056c59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images...\n",
      "Discriminating images...\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 200\n",
    "D = DiscrimiNet()\n",
    "G = GeneratioNet()\n",
    "loss = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "if disc_loss is None:\n",
    "    disc_loss = []\n",
    "if genr_loss is None:\n",
    "    genr_loss = []\n",
    "\n",
    "generator_iter = 0\n",
    "\n",
    "epoch=1\n",
    "\n",
    "i, (images, _) = next(enumerate(train_loader))\n",
    "\n",
    "# Step 1: Train discriminator\n",
    "z = torch.randn(batch_size, 100, 1, 1)\n",
    "\n",
    "real_labels = torch.ones(batch_size)\n",
    "fake_labels = torch.zeros(batch_size)\n",
    "\n",
    "images, z = images.to(device), z.to(device)\n",
    "real_labels, fake_labels = real_labels.to(device), fake_labels.to(device)\n",
    "\n",
    "## Compute the BCE Loss using real images\n",
    "real_logits = D(images)\n",
    "real_logits = torch.squeeze(real_logits)\n",
    "d_loss_real = loss(real_logits, real_labels)\n",
    "\n",
    "## Compute the BCE Loss using fake images\n",
    "print(\"Generating images...\")\n",
    "fake_images = G(z)\n",
    "print(\"Discriminating images...\")\n",
    "fake_logits = D(fake_images)\n",
    "fake_logits = torch.squeeze(fake_logits)\n",
    "d_loss_fake = loss(fake_logits, fake_labels)\n",
    "\n",
    "## Optimize discriminator\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "D.zero_grad()\n",
    "d_loss.backward()\n",
    "d_optimizer.step()\n",
    "\n",
    "## Step 2: Train Generator\n",
    "z = torch.randn(batch_size, 100, 1, 1).to(device)\n",
    "\n",
    "fake_images = G(z)\n",
    "fake_logits = D(fake_images)\n",
    "fake_logits = torch.squeeze(fake_logits)\n",
    "g_loss = loss(fake_logits, real_labels)\n",
    "\n",
    "D.zero_grad()\n",
    "G.zero_grad()\n",
    "g_loss.backward()\n",
    "g_optimizer.step()\n",
    "generator_iter += 1\n",
    "\n",
    "#disc_loss.append(d_loss.item())\n",
    "#genr_loss.append(g_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bea9f9-ab0d-44c1-87b6-b7f07b239682",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Batch 1 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 2 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 3 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 4 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 5 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 6 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 7 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 8 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 9 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 10 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 11 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 12 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 13 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 14 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 15 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 16 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 17 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 18 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 19 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 20 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 21 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 22 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 23 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 24 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 25 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 26 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 27 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 28 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 29 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 30 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 31 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 32 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 33 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 34 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 35 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 36 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 37 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 38 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 39 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 40 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 41 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 42 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 43 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 44 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 45 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 46 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 47 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 48 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 49 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 50 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 51 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 52 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 53 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 54 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 55 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 56 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 57 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 58 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 59 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 60 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 61 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 62 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 63 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 64 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 65 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 66 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 67 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 68 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 69 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 70 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 71 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 72 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 73 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 74 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 75 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 76 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 77 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 78 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 79 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 80 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 81 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 82 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 83 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 84 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 85 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 86 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 87 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 88 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 89 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 90 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 91 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 92 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 93 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 94 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 95 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 96 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 97 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 98 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 99 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 100 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 101 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 102 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 103 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 104 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 105 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 106 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 107 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 108 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 109 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 110 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 111 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 112 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 113 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 114 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 115 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 116 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 117 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 118 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 119 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 120 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 121 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 122 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 123 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 124 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 125 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 126 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 127 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 128 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 129 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n",
      "Epoch 1: Batch 130 of 300\n",
      "Generating images...\n",
      "Discriminating images...\n"
     ]
    }
   ],
   "source": [
    "# set number_of_images, z\n",
    "number_of_images = 1\n",
    "z = torch.randn(batch_size, 100)\n",
    "\n",
    "# set up model\n",
    "# run training\n",
    "model = DCGANet(epochs, batch_size)\n",
    "disc_loss, genr_loss = model.train(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a96dd7-f87a-4bf7-bd76-1ff6e251e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.G.state_dict(), \"GAN_g.pth\")\n",
    "torch.save(model.D.state_dict(), \"GAN_d.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d744f911-1979-4255-946c-fc96de7b6830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = DCGANet(epochs, batch_size)\n",
    "model.G.load_state_dict(torch.load('GAN_g.pth'))\n",
    "model.D.load_state_dict(torch.load('GAN_d.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58506440-f7a5-4376-a1ac-8ce9481dcba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.rand((batch_size, 100, 1, 1)).to(device)\n",
    "out = model.G(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d73c6d-5a7d-4cc1-95a8-7c732812affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = out[0,0,:,:].detach().numpy()\n",
    "img2 = out[1,0,:,:].detach().numpy()\n",
    "img3 = out[2,0,:,:].detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,5))\n",
    "ax.plot(range(len(disc_loss)), disc_loss, marker='', label=\"discriminator\")\n",
    "ax.plot(range(len(genr_loss)), genr_loss, marker='', label=\"generator\")\n",
    "ax.legend()\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize=(15,15))\n",
    "ax[0].imshow(img1)\n",
    "ax[1].imshow(img2)\n",
    "ax[2].imshow(img3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba36e81-ceee-49a5-b7a9-b6b42b6b0d47",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "b248f1b5-e783-4943-8a05-448579f5616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata, copy\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "#download corpus\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "2272fd2c-815a-44fc-b48c-0dab699b472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_text(text):\n",
    "    #remove html strips\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    #remove \\n\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    #remove square brackets\n",
    "    text = re.sub('\\[[^]]*\\]', '', text)\n",
    "    #replace punctuation with space\n",
    "    text = re.sub(r'[,.;@#?!&$\\-]+\\ *', ' ', text, flags=re.VERBOSE)\n",
    "    #remove special characters\n",
    "    text=re.sub(r'[^a-zA-z0-9\\s]', '', text)\n",
    "    #remove extra spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "def make_optimizer(optimizer_name, model, **kwargs):\n",
    "    if optimizer_name=='Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=kwargs['lr'])\n",
    "    elif optimizer_name=='SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(),lr=kwargs['lr'],momentum=kwargs['momentum'], weight_decay=kwargs['weight_decay'])\n",
    "    else:\n",
    "        raise ValueError('Not valid optimizer name')\n",
    "    return optimizer\n",
    "    \n",
    "def make_scheduler(scheduler_name, optimizer, **kwargs):\n",
    "    if scheduler_name=='MultiStepLR':\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=kwargs['milestones'],gamma=kwargs['factor'])\n",
    "    else:\n",
    "        raise ValueError('Not valid scheduler name')\n",
    "    return scheduler\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, epoch):\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    for batch_idx, (grams, targets) in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(grams).squeeze(1)\n",
    "        loss = criterion(output.float(), targets.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if batch_idx % (len(iterator)//4) == 0:\n",
    "            print('Train({})[{:.0f}%]: Loss: {:.4f}'.format(\n",
    "                epoch, 100. * batch_idx / len(iterator), train_loss/(batch_idx+1)))\n",
    "    print(f\"Epoch {epoch} complete!\")\n",
    "            \n",
    "    return model\n",
    "\n",
    "def text_to_index(text, d):\n",
    "    corpus = torch.Tensor([d[word] for word in text])\n",
    "    return corpus\n",
    "\n",
    "def index_to_text(index, unique_words):\n",
    "    return unique_words[index.int().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "1b7e0803-ee0d-40a3-8236-5532cabc816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "\n",
    "class TextPredictioNet(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, n_classes, n_layers, e_dim):\n",
    "        super(TextPredictioNet, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.n_layers = n_layers\n",
    "        self.e_dim = e_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.n_classes, \n",
    "                                      self.e_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = self.e_dim, \n",
    "                                  hidden_size = self.h_dim, \n",
    "                                  num_layers = self.n_layers, \n",
    "                                  batch_first=True)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=self.h_dim, \n",
    "                                    out_features=self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.int()\n",
    "        \n",
    "        # initialize H and C\n",
    "        h0 = ((torch.rand((self.n_layers, len(x), self.h_dim)) - 0.5)*5).to(device)\n",
    "        c0 = ((torch.rand((self.n_layers, len(x), self.h_dim)) - 0.5)*5).to(device)\n",
    "        \n",
    "        x_embed = self.embedding(x)\n",
    "        m, _ = self.lstm(x_embed, (h0, c0))\n",
    "        \n",
    "        logits = self.classifier(m[:, -1, :])\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "fb363481-b3ee-4bf7-8287-b8191b503b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "corpus_raw = nltk.corpus.gutenberg.raw('austen-sense.txt')\n",
    "\n",
    "# clean dataset\n",
    "corpus_raw = denoise_text(corpus_raw)\n",
    "\n",
    "# tokenize\n",
    "corpus_t = nltk.word_tokenize(corpus_raw)\n",
    "\n",
    "# convert to one-hot encoding\n",
    "unique_words = np.unique(corpus_t)\n",
    "d = {word: i for i, word in enumerate(unique_words)}\n",
    "corpus = text_to_index(corpus_t, d)\n",
    "\n",
    "# generate 6-grams\n",
    "# split into 5-grams and targets\n",
    "grams_pre = torch.stack(\n",
    "    [corpus[i:i+6] for i in range(len(corpus)-6+1)], \n",
    "    dim=-1).permute(1,0)\n",
    "grams = grams_pre[:,:5].float()\n",
    "targets = grams_pre[:,5].float()\n",
    "\n",
    "# pad dataset to N=100\n",
    "#grams = torch.cat((grams,\n",
    "#                     torch.zeros(len(grams), 95, len(unique_words)),\n",
    "#                     dim=1)\n",
    "# too big ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "4571448d-3c57-4ea6-8875-21aa73ed0c5c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Train(1)[0%]: Loss: 8.7584\n",
      "Train(1)[25%]: Loss: 244.3838\n",
      "Train(1)[50%]: Loss: 318.5965\n",
      "Train(1)[75%]: Loss: 364.0584\n",
      "Train(1)[100%]: Loss: 404.3141\n",
      "Epoch 1 complete!\n",
      "Starting training...\n",
      "Train(2)[0%]: Loss: 563.1417\n",
      "Train(2)[25%]: Loss: 535.4905\n",
      "Train(2)[50%]: Loss: 533.8349\n",
      "Train(2)[75%]: Loss: 534.9758\n",
      "Train(2)[100%]: Loss: 544.5876\n",
      "Epoch 2 complete!\n",
      "Starting training...\n",
      "Train(3)[0%]: Loss: 584.0623\n",
      "Train(3)[25%]: Loss: 563.8273\n",
      "Train(3)[50%]: Loss: 558.0367\n",
      "Train(3)[75%]: Loss: 555.3468\n",
      "Train(3)[100%]: Loss: 561.9745\n",
      "Epoch 3 complete!\n",
      "Starting training...\n",
      "Train(4)[0%]: Loss: 570.4008\n",
      "Train(4)[25%]: Loss: 578.4389\n",
      "Train(4)[50%]: Loss: 570.9822\n",
      "Train(4)[75%]: Loss: 568.2605\n",
      "Train(4)[100%]: Loss: 573.8702\n",
      "Epoch 4 complete!\n",
      "Starting training...\n",
      "Train(5)[0%]: Loss: 640.1093\n",
      "Train(5)[25%]: Loss: 587.9116\n",
      "Train(5)[50%]: Loss: 579.7801\n",
      "Train(5)[75%]: Loss: 573.9970\n",
      "Train(5)[100%]: Loss: 577.7585\n",
      "Epoch 5 complete!\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# build data loader\n",
    "gram_set = TensorDataset(grams, targets)\n",
    "dl = DataLoader(gram_set, batch_size=256)\n",
    "\n",
    "# instantiate model\n",
    "in_dim = len(unique_words)\n",
    "h_dim = 1000\n",
    "n_classes = len(unique_words)\n",
    "n_layers = 1\n",
    "e_dim = 50\n",
    "\n",
    "model = TextPredictioNet(in_dim, h_dim, n_classes, n_layers, e_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = make_optimizer(\"Adam\", model, lr=1, momentum=0, weight_decay=1)\n",
    "scheduler = make_scheduler(\"MultiStepLR\", optimizer, milestones=[5], factor=0.1)\n",
    "\n",
    "# run training loop\n",
    "for epoch in range(5):\n",
    "    model = train(model, dl, optimizer, criterion, epoch+1)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "id": "c71350cd-575e-4072-9aa5-250f9d3846c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "his natural shyness was overcome by for a having sudden so by ungraciously a protestations of of a each of hardly colonel and brandon of would edward have i some can failing desire well confined and at with last the autumn only dried of cherries mind and and to here allow ceased and in rate bringing she to therefore give a he suspicion might of enjoyment mrs have ferrars some am painful now in during hearing and every who thing told she friend was no allowed injury wondered may to to take mrs her ferrars daughter as them happy leisure no robert married which and\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "text = [\"his\", \"natural\", \"shyness\", \"was\", \"overcome\"]\n",
    "inds = text_to_index(text, d)\n",
    "\n",
    "for i in range(100):\n",
    "    inds_sub = inds[-5:-1].unsqueeze(0)\n",
    "    pred = torch.argmax(model(inds_sub)).unsqueeze(0)\n",
    "    inds = torch.cat((inds, pred), dim=0)\n",
    "\n",
    "text_pred = \" \".join(index_to_text(inds, unique_words))\n",
    "print(text_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf4f30-9ae0-49cc-9e52-ed4ad4606f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1080dda3-baa7-49f6-8b92-d67b512f7ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
