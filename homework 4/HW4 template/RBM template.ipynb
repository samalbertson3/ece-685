{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The material presented in this jupyter notebook is part of the \"ECE 685D: Introduction to Deep Learning\" course offered at Duke University.\n",
    "# Session: Fall 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8a: Restricted Boltzmann Machine (RBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RBM.jpg\" width=340 height=340 />\n",
    "\n",
    "RBM is an undirected graphical model for unsupervised learning. In this session, we will consider stochastic binary visible variables: $x\\in\\{0,1\\}^D$ and stochastic binary hidden variables: $h \\in \\{0,1\\}^H$ to model MNIST images. This will involve training a Bernoulli-Bernoulli RBM for generating images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites: Lecture 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall:\n",
    "\n",
    "$$P(x, h) = \\frac{exp(-E(x, h))}{Z}$$\n",
    "\n",
    "where $E(x,h)$ is the energy associated with the joint configuration of (x,h). In the Bernoulli-Bernoulli RBM, $E(x, h) := -h^TWx - c^Tx - b^Th$.\n",
    "\n",
    "Using the above, we can determine the following:\n",
    "\n",
    "* Marginal of x: $P(x) = \\frac{exp(c^Tx + \\sum_{j=1}^H softplus{(b_j + W_{j.}x)} )}{Z} = \\frac{exp(-F(x))}{Z}$, where $F(x)$ is called the \"free energy\".\n",
    "* Conditionals: $p(x|h)$ and $p(h|x)$.\n",
    "\n",
    "Note: $Z = \\sum_{x,h} exp(-E(x,h)) = \\sum_x exp(-F(x)) $\n",
    "\n",
    "Objective: Minimize the Negative log-likelihood $-ln(P(x))$ of training samples:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-lnP(x) &= F(x) + ln(Z) \\\\ \\\\\n",
    "% -\\frac{\\partial lnP(x)}{\\partial \\theta} &= \\frac{\\partial F(x)}{\\partial \\theta} + \\frac{\\partial lnZ}{\\partial \\theta} \\\\\n",
    "-\\frac{\\partial lnP(x)}{\\partial \\theta} &= \\frac{\\partial F(x)}{\\partial \\theta} - \\sum_{\\tilde{x}} p(\\tilde{x}) \\frac{\\partial F(\\tilde{x})}{\\partial \\theta} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\tilde{x}$ denotes the negative sample. To sample $\\tilde{x}$, we use MCMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contrastive Divergence (CD-k)**\n",
    "\n",
    "Contrastive Divergence uses two tricks to speed up the sampling process:\n",
    "\n",
    "* We initialize the Markov chain with a training example.\n",
    "* CD does not wait for the chain to converge. Samples are obtained after only k-steps of Gibbs sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Train Bernoulli-Bernoulli RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_set, test_set, train_loader, test_loader = {},{},{},{}\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "train_set['mnist'] = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set['mnist'] = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader['mnist'] = torch.utils.data.DataLoader(train_set['mnist'], batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader['mnist'] = torch.utils.data.DataLoader(test_set['mnist'], batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(nn.Module):\n",
    "    \"\"\"Restricted Boltzmann Machine for generating MNIST images.\"\"\"\n",
    "    \n",
    "    def __init__(self, D: int, F: int, k: int):\n",
    "        \"\"\"Creates an instance RBM module.\n",
    "            \n",
    "            Args:\n",
    "                D: Size of the input data.\n",
    "                F: Size of the hidden variable.\n",
    "                k: Number of MCMC iterations for negative sampling.\n",
    "                \n",
    "            The function initializes the weight (W) and biases (c & b).\n",
    "        \"\"\"\n",
    "        \n",
    "        super(RBM, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(F, D)* 1e-2) # Initialized from Normal(mean=0.0, variance=1e-4)\n",
    "        self.c = nn.Parameter(torch.zeros(D)) # Initialized as 0.0\n",
    "        self.b = nn.Parameter(torch.zeros(F)) # Initilaized as 0.0\n",
    "        self.k = k\n",
    "    \n",
    "    def sample(self, p):\n",
    "        \"\"\"Sample from a bernoulli distribution defined by a given parameter.\n",
    "        \n",
    "           Args:\n",
    "                p: Parameter of the bernoulli distribution.\n",
    "           \n",
    "           Returns:\n",
    "               bern_sample: Sample from Bernoulli(p)\n",
    "        \"\"\"\n",
    "        \n",
    "        bern_sample = p.bernoulli()\n",
    "        return bern_sample\n",
    "    \n",
    "    def P_h_x(self, x):\n",
    "        \"\"\"Returns the conditional P(h|x). (Slide 9, Lecture 14)\n",
    "        \n",
    "        Args:\n",
    "            x: The parameter of the conditional h|x.\n",
    "        \n",
    "        Returns:\n",
    "            ph_x: probability of hidden vector being element-wise 1.\n",
    "        \"\"\"\n",
    "\n",
    "        ph_x = torch.sigmoid(F.linear(x, self.W, self.b)) # n_batch x F\n",
    "        return ph_x\n",
    "    \n",
    "    def P_x_h(self, h):\n",
    "        \"\"\"Returns the conditional P(x|h). (Slide 9, Lecture 14)\n",
    "        \n",
    "        Args:\n",
    "            h: The parameter of the conditional x|h.\n",
    "        \n",
    "        Returns:\n",
    "            px_h: probability of visible vector being element-wise 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        px_h = torch.sigmoid(F.linear(h, self.W.t(), self.c)) # n_batch x D\n",
    "        return px_h\n",
    "\n",
    "    def free_energy(self, x):\n",
    "        \"\"\"Returns the Average F(x) free energy. (Slide 11, Lecture 14).\"\"\"\n",
    "        \n",
    "        vbias_term = x.mv(self.c) # n_batch x 1\n",
    "        wv_b = F.linear(x, self.W, self.b) # n_batch x F\n",
    "        hidden_term = F.softplus(wv_b).sum(dim=1) # n_batch x 1\n",
    "        return (-hidden_term - vbias_term).mean() # 1 x 1 \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Generates x_negative using MCMC Gibbs sampling starting from x.\"\"\"\n",
    "        \n",
    "        x_negative = x\n",
    "        for _ in range(self.k):\n",
    "            \n",
    "            ## Step 1: Sample h from previous iteration.\n",
    "            # Get the conditional prob h|x\n",
    "            phx_k = self.P_h_x(x_negative) \n",
    "            # Sample from h|x\n",
    "            h_negative = self.sample(phx_k)\n",
    "            \n",
    "            ## Step 2: Sample x using h from step 1.\n",
    "            # Get the conditional proba x|h\n",
    "            pxh_k = self.P_x_h(h_negative)\n",
    "            # Sample from x|h\n",
    "            x_negative = self.sample(pxh_k)\n",
    "\n",
    "        return x_negative, pxh_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # torchvision provides us with normalized data, s.t. input is in [0,1]\n",
    "        data = data.view(data.size(0),-1) # flatten the array: Converts n_batchx1x28x28 to n_batchx784\n",
    "        data = data.bernoulli() \n",
    "        data = data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_tilde, _ = model(data)\n",
    "        x_tilde = x_tilde.detach()\n",
    "        \n",
    "        loss = model.free_energy(data) - model.free_energy(x_tilde)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if (batch_idx+1) % (len(train_loader)//2) == 0:\n",
    "            print('Train({})[{:.0f}%]: Loss: {:.4f}'.format(\n",
    "                epoch, 100. * batch_idx / len(train_loader), train_loss/(batch_idx+1)))\n",
    "\n",
    "def test(model, device, test_loader, epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.view(data.size(0),-1)\n",
    "            data = data.bernoulli()\n",
    "            data = data.to(device)\n",
    "            xh_k,_ = model(data)\n",
    "            loss = model.free_energy(data) - model.free_energy(xh_k)\n",
    "            test_loss += loss.item() # sum up batch loss\n",
    "    \n",
    "    test_loss = (test_loss*batch_size)/len(test_loader.dataset)\n",
    "    print('Test({}): Loss: {:.4f}'.format(epoch, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Define make_optimizer and make_schedule__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(optimizer_name, model, **kwargs):\n",
    "    if optimizer_name=='Adam':\n",
    "        optimizer = optim.Adam(model.parameters(),lr=kwargs['lr'])\n",
    "    elif optimizer_name=='SGD':\n",
    "        optimizer = optim.SGD(model.parameters(),lr=kwargs['lr'],momentum=kwargs.get('momentum', 0.), \n",
    "                              weight_decay=kwargs.get('weight_decay', 0.))\n",
    "    else:\n",
    "        raise ValueError('Not valid optimizer name')\n",
    "    return optimizer\n",
    "    \n",
    "def make_scheduler(scheduler_name, optimizer, **kwargs):\n",
    "    if scheduler_name=='MultiStepLR':\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer,milestones=kwargs['milestones'],gamma=kwargs['factor'])\n",
    "    else:\n",
    "        raise ValueError('Not valid scheduler name')\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General variables\n",
    "\n",
    "seed = 1\n",
    "data_name = 'mnist'\n",
    "optimizer_name = 'Adam'\n",
    "scheduler_name = 'MultiStepLR'\n",
    "num_epochs = 10\n",
    "lr = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(device)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "rbm = RBM(D=28*28, F=400, k=5).to(device)\n",
    "optimizer = make_optimizer(optimizer_name, rbm, lr=lr)\n",
    "scheduler = make_scheduler(scheduler_name, optimizer, milestones=[5], factor=0.1)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    \n",
    "    train(rbm, device, train_loader[data_name], optimizer, epoch)\n",
    "    test(rbm, device, test_loader[data_name], epoch)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print('Optimizer Learning rate: {0:.4f}\\n'.format(optimizer.param_groups[0]['lr']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img1, img2):\n",
    "    npimg1 = img1.cpu().numpy()\n",
    "    npimg2 = img2.cpu().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(1,2, figsize=(20,10))\n",
    "    axes[0].imshow(np.transpose(npimg1, (1,2,0)), interpolation='nearest')\n",
    "    axes[1].imshow(np.transpose(npimg2, (1,2,0)), interpolation='nearest')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot original images and reconstructed images using the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,_ = next(iter(test_loader[data_name]))\n",
    "data = data[:32]\n",
    "data_size = data.size()\n",
    "data = data.view(data.size(0),-1)\n",
    "bdata = data.bernoulli().to(device)\n",
    "vh_k, pvh_k = rbm(bdata)\n",
    "vh_k, pvh_k = vh_k.detach(), pvh_k.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(make_grid(data.reshape(data_size), padding=0), make_grid(pvh_k.reshape(data_size), padding=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
