{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa0128c7-b71c-40ca-bff1-17cb2017a9e7",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41159fe-d5f8-4d84-a888-66b3716237a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3037e831-b03b-48dd-b602-4c1071ddcbfe",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa77913c-5657-4e4a-8b5a-fa43b8a35b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in fashion MNIST dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "\n",
    "# download data\n",
    "train = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# split data into training/test\n",
    "# split training data into training/validation\n",
    "# set up data loaders\n",
    "train_split = int(0.8 * len(train))\n",
    "val_split = len(train) - train_split\n",
    "train, val = random_split(train, [train_split, val_split])\n",
    "train_loader = DataLoader(train, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=64, shuffle=True)\n",
    "\n",
    "# labels for later\n",
    "master_labels = [\"T-shirt\", \"Pants\", \"Pullover\" , \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d445363-f2c9-4345-b85e-2b63986e2414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-shirt\n",
      "torch.Size([1, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdfa7a14f10>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, lab = next(iter(train_loader))\n",
    "print(master_labels[lab])\n",
    "print(img.shape)\n",
    "plt.imshow(img.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c970e6c3-c74f-4de9-b9a6-eed4b993beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build autoencoder\n",
    "\n",
    "class FashioNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CorruptioNet, self).__init__()\n",
    "        \n",
    "        self.Encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 7, stride=1, padding=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(16, 8, 5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        self.Decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ConvTranspose2d(16, 1, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        self.c1 = nn.ConvTranspose2d(1, 1, 1, stride=1, padding=0)\n",
    "        self.a1 = nn.Sigmoid()\n",
    "        self.bn1 = nn.BatchNorm2d(1)\n",
    "        self.d1 = nn.Dropout(0.5)\n",
    "        \n",
    "    def Decoder2(self, x, x_old):\n",
    "        x = self.c1(x + x_old)\n",
    "        #x = self.c1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.a1(x)\n",
    "        x = torch.round(x * 255) # rescaling to RGB range\n",
    "        return(x)\n",
    "    \n",
    "    def forward(self, x, lab):\n",
    "        x_old = x\n",
    "        x_r = x[:,0,:,:].unsqueeze(1)\n",
    "        x_g = x[:,1,:,:].unsqueeze(1)\n",
    "        x_b = x[:,2,:,:].unsqueeze(1)\n",
    "        \n",
    "        x_r = self.Encoder(x_r)\n",
    "        x_g = self.Encoder(x_g)\n",
    "        x_b = self.Encoder(x_b)\n",
    "        \n",
    "        x_r = self.Decoder(x_r)\n",
    "        x_g = self.Decoder(x_g)\n",
    "        x_b = self.Decoder(x_b)\n",
    "        \n",
    "        x_r = self.Decoder2(x_r, x_old[:,0,:,:].unsqueeze(1))\n",
    "        x_g = self.Decoder2(x_g, x_old[:,1,:,:].unsqueeze(1))\n",
    "        x_b = self.Decoder2(x_b, x_old[:,2,:,:].unsqueeze(1))\n",
    "        \n",
    "        x = torch.cat((x_r, x_g, x_b), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fbeaf062-5f8a-4fa1-9bff-ba8c087b1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training function\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epoch):\n",
    "    start = time()\n",
    "    \n",
    "    # define loss function\n",
    "    # set optimizer\n",
    "    #criterion = nn.MSELoss()\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-11, weight_decay=0)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # run training loop\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    min_loss = 0\n",
    "    i = 0\n",
    "    stop_flag = False\n",
    "    for batch_id, (img, lab) in enumerate(train_loader):\n",
    "        img = img.to(device)\n",
    "        lab = lab.to(device)\n",
    "\n",
    "        # training loss + gradient descent\n",
    "        print(f\"\"\"Batch {batch_id}/{len(train_loader)}\"\"\", end=\"\\r\")\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out_image = model(img, lab)\n",
    "        \n",
    "        loss = criterion(img, out_image)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append([epoch, batch_id, loss.item()/N_train])\n",
    "\n",
    "        # validation loss\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        N = 0\n",
    "        with torch.no_grad():\n",
    "            for _, (img, lab) in enumerate(val_loader):\n",
    "                lab = lab.to(device)\n",
    "                img = img.to(device)\n",
    "\n",
    "                N_batch = corr.size(0)\n",
    "                N += N_batch\n",
    "                \n",
    "                out_image = model(img, lab)\n",
    "                \n",
    "                loss = criterion(img, out_image)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                break\n",
    "        val_loss = val_loss/N\n",
    "        val_losses.append([epoch, batch_id, val_loss])\n",
    "\n",
    "        # save models periodically\n",
    "        if (i % 50 == 0) & (i > 1):\n",
    "            old_model = model\n",
    "            \n",
    "            # show test image\n",
    "            losses2 = np.array(losses)\n",
    "            val_losses2 = np.array(val_losses)\n",
    "            fig, ax = plt.subplots(1,3,figsize=(10,5))\n",
    "            ax[0].plot(losses2[:,1], losses2[:,2], marker='', label=\"training\")\n",
    "            ax[0].plot(val_losses2[:,1], val_losses2[:,2], marker='', label=\"validation\")\n",
    "\n",
    "            _, (img, lab) = next(enumerate(val_loader))\n",
    "            out_image = model(img, lab)\n",
    "            plt.imshow(out_image)\n",
    "            \n",
    "            # early stopping\n",
    "            if i > 5:\n",
    "                recent_loss = np.mean(np.array(val_losses)[-50:-1,2])\n",
    "                if min_loss == 0:\n",
    "                    min_loss = recent_loss\n",
    "                elif recent_loss < min_loss:\n",
    "                    min_loss = recent_loss\n",
    "                elif recent_loss > min_loss:\n",
    "                    stop_flag = True\n",
    "                    return old_model, losses, val_losses, stop_flag\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    print(time() - start)\n",
    "    return model, losses, val_losses, stop_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f8acc39-257a-4881-987a-17f27fa7d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 10 images per class (dress, coat, sandal, sneaker)\n",
    "# generate manifold plot (tips in homework)\n",
    "\n",
    "label_embed = torch.zeros(img.shape[0], 10, img.shape[2], img.shape[3], device=img.device)\n",
    "label_embed[range(img.shape[0]), lab] = 1.0  # One-hot encoding for labels\n",
    "\n",
    "img = torch.cat((img, label_embed), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "269c02c2-06ea-4655-9865-30f85b606d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 28, 28])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629dad2d-dfd3-4909-a70e-50d115679cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
